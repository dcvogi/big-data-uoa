In this section we describe in detail the methodology used in order to detect duplicate documents.
\subsubsection{Vectorizer}
In order to compute the similarity of the documents, we must first  produce the term vectors for each document of the dataset (test set). To do that we used \emph{TfidfVectorizer} from Python sklearn library. TfidfVectorizer transforms a collection of raw documents to a matrix of TF-IDF features. TF-IDF, short for term frequency-inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. \cite{rajaraman2011datamining} The TF-IDF formula is defined by the following equation\cite{victor2014csmr}\cite{evangelopoulos2016evaluating}:
\begin{equation}
	tfidf(i, j) = \frac{n_{i, j}}{|t \in d_j|} \cdot \log \frac{|D|}{|d \in D : t \in d|}
\end{equation}
Where $i$ the term index,  $j$ the document index, $d$ a document, $t$ a single term and $D$ the whole document corpus.
\subsubsection{Similarity detection}
At this stage and after we produced the term vectors with TfidfVectorizer, we compute the cosine similarity between the  vectors of each document versus the others, ending up with a $n \times n$  matrix where the value of the pair $i, j$ corresponds to the cosine of the angle between. We consider this angle as the document similarity. In order to avoid double-checking the same pairs, we traverse the upper triangular of the matrix keeping only the pairs with similarity greater than the input threshold $\theta$. 